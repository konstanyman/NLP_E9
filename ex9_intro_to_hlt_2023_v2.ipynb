{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkuNLP/intro-to-nlp/blob/master/ex9_intro_to_hlt_2023_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF1-YWS82oKK"
      },
      "source": [
        "**Watch out, this notebook stretches colab memory with n=5, so you might need to \"Restart and run all\" on full re-runs of the notebook,\n",
        "since the old data clogs the memory during a rerun**\n",
        "\n",
        "In this exercise, you'll try to generate text with an n-gram model. In the generation, we use the last generated n-1 words as the prefix, and the n-gram counts to establish the distribution of possible continuations. So we might run this off the following data structure:\n",
        "\n",
        "* A master dictionary, where the key are n-1 grams\n",
        "* The value is another dictionary\n",
        "* In this dictonary the key is a word\n",
        "* And the value is its count\n",
        "\n",
        "So, when generating, we can take the last n-1 words, look them up in the master dictionary, and we get a dictionary of all seen continuations and their counts.\n",
        "\n",
        "Let us divide it to the following tasks:\n",
        "\n",
        "1. Generate n-grams from a corpus of text, e.g. the IMDB dataset\n",
        "2. Count the n-grams, i.e. build the master dictionary\n",
        "\n",
        "With these data structures, the generation can proceed quite easily. Say, we have a 4-gram model.\n",
        "\n",
        "* Given a prior context $w_1w_2w_3$\n",
        "* Look up the word-count dictionary of possible words $w_4$\n",
        "* The counts, once normalized to sum up to 1, form a distribution over words that can continue $w_1w_2w_3$ and we can sample the next word from this distribution.\n",
        "* Then we append this generated word to our list of already generated words, and repeat the process\n",
        "\n",
        "\n",
        "Other remarks:\n",
        "\n",
        "* We want to pad all texts with `<bos>` (beginning of sequence) and <eos> (end of sequence). The `<bos>` we want to have there n-1 times, so we can use it as the initial prompt and let the model learn how the sequences start. The `<eos>` allows us to stop generating, and prevents a crash on unknown n-grams at the very end of a sequence. (if an n-gram $w_1w_2w_3w_4$ was seen only once at the end of a \"training\" sequence, then an attempt to continue it during generation, would lead to a crash, since we have no known n-gram to continue the sequence $w_2w_3w_4$ with our simple, unsmoothed model :)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD9E2rDD7oyE"
      },
      "source": [
        "# Task A: Generate n-grams\n",
        "\n",
        "* Write a generator function (using `yield` rather than `return`) which yields n-grams as tuples $(w_1,...,w_n)$ from all sections of the IMDB dataset\n",
        "* a vectorizer from `sklearn` can be used as a trivial tokenizer\n",
        "* `more-itertools` is a nifty library to achieve the n-gram generation\n",
        "* remember to pad with n-1 `<bos>` symbols at the beginning, and one `<eos>` symbol at the end\n",
        "\n",
        "You can give this a shot, or simply use the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4efOx1BpIN9",
        "outputId": "88b0dbdd-9c55-4c04-bd5e-8f34f936a48f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in c:\\python310\\lib\\site-packages (2.18.0)\n",
            "Requirement already satisfied: more-itertools in c:\\python310\\lib\\site-packages (10.2.0)\n",
            "Requirement already satisfied: aiohttp in c:\\python310\\lib\\site-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in c:\\python310\\lib\\site-packages (from datasets) (0.21.4)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\python310\\lib\\site-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pandas in c:\\python310\\lib\\site-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in c:\\python310\\lib\\site-packages (from datasets) (15.0.0)\n",
            "Requirement already satisfied: multiprocess in c:\\python310\\lib\\site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\python310\\lib\\site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.19.0 in c:\\users\\konst\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in c:\\python310\\lib\\site-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in c:\\python310\\lib\\site-packages (from datasets) (2024.2.0)\n",
            "Requirement already satisfied: packaging in c:\\python310\\lib\\site-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: filelock in c:\\python310\\lib\\site-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\konst\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: xxhash in c:\\python310\\lib\\site-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in c:\\python310\\lib\\site-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (3.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\python310\\lib\\site-packages (from huggingface-hub>=0.19.4->datasets) (4.8.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: colorama in c:\\python310\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\python310\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\python310\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\python310\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -3p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -2p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -1p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -0p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -3p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -2p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -1p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -0p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -3p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -2p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -1p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -0p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -3p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -2p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -1p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -0p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -3p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -2p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -1p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -0p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -3p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -2p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -1p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -0p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install datasets more-itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WklQqyCFo_aq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import datasets\n",
        "import sklearn.feature_extraction\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "6cb8849da4eb4de5834cae0da4ac81ba",
            "64c545796eab4071903ef80275e2b2d6",
            "729e5eac79df4812a01397dbd3d3e22d",
            "9b29d52a6b734c73ac8362c36fd8dd48",
            "6ccc79e5d40d4438baf86cb38b2705af",
            "696fae99698740249a27c31e20303c60",
            "c14be0435fd84bdb8991b654c81b3045",
            "9dfb87ed187143f0b6ca59d3fffd7224",
            "5cda0e27d1b6445b9b713e6eccd46012",
            "21c661ddcf174e599811a3cd9513ca11",
            "01a4d87ea5a54f16bbeae72894c85632"
          ]
        },
        "id": "BAiGkrtqpYo1",
        "outputId": "cd530b6b-6d4f-4124-86b1-b59d103f911b"
      },
      "outputs": [],
      "source": [
        "dset=datasets.load_dataset(\"imdb\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeKF44p4pjyE",
        "outputId": "c126fc77-e32c-4df1-ba26-71522fe5e9ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['I',\n",
              " 'have',\n",
              " 'a',\n",
              " 'dog',\n",
              " 'at',\n",
              " 'home',\n",
              " 'it',\n",
              " 'likes',\n",
              " 'to',\n",
              " 'shred',\n",
              " 'newspapers']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Few remarks here:\n",
        "# 1. we don't need the vectorizer per se, we just want its analyzer function, which basically tokenizes the text for us, and somewhat unfortunately drops punctuation\n",
        "# 2. the default token pattern in sklearn drops 1-letter words (like \"I\" and \"a\") so I modify it a bit\n",
        "# 3. it's a pretty lousy tokenizer, but it will do for this toy exercise\n",
        "cvectorizer=sklearn.feature_extraction.text.CountVectorizer(lowercase=False,stop_words=None,token_pattern=r\"(?u)\\b\\w+\\b\" )\n",
        "analyzer=cvectorizer.build_analyzer()\n",
        "analyzer(\"I have a dog at home, it likes to shred newspapers.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCdn2w-drcVN",
        "outputId": "04c4d74a-213d-405b-fce6-357f832ef130"
      },
      "outputs": [],
      "source": [
        "# Now we tokenize the IMDB dataset the usual way\n",
        "def tokenize(ex):\n",
        "    return {\"tokenized\":analyzer(ex[\"text\"])}\n",
        "\n",
        "dset=dset.map(tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OPXGteLQtOXD"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from more_itertools import sliding_window #more-itertools is an awesome library!\n",
        "import tqdm\n",
        "\n",
        "def generate_ngrams(dset,n):\n",
        "    for ex in tqdm.tqdm(dset):\n",
        "        tokens=[\"<bos>\"]*(n-1)+ex[\"tokenized\"]+[\"<eos>\"]\n",
        "        for ngram in sliding_window(tokens,n):\n",
        "            yield ngram\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOoapsbtAcyM"
      },
      "source": [
        "# Task B\n",
        "\n",
        "* Now we can combine the different sections of the IMDB dataset and count our n-grams\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UiuqC8yl_lqo"
      },
      "outputs": [],
      "source": [
        "# Here we can concatenate all the individual datasets (train,test,unlabeled) in IMDB\n",
        "# the \"master\" dataset is a dictionary of these, so dset.values() has the datasets of the individual sections (train,test,unlabeled)\n",
        "combined_dataset=datasets.concatenate_datasets(list(dset.values()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4qumygtAYZH",
        "outputId": "de4e8a47-1500-44d3-e749-0471fc1b7c23"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100000/100000 [01:09<00:00, 1440.72it/s]\n"
          ]
        }
      ],
      "source": [
        "ngrams={} #This is the master dictionary\n",
        "for ngram in generate_ngrams(combined_dataset,4): #let's start with 4-grams, you can try 2- 3- and 5- grams too!\n",
        "    if ngram[:3] not in ngrams:\n",
        "        ngrams[ngram[:3]] = {ngram[3]: 1}\n",
        "        continue\n",
        "    if ngram[3] not in ngrams[ngram[:3]]:\n",
        "        ngrams[ngram[:3]].update({ngram[3]: 1})\n",
        "    else:\n",
        "        ngrams[ngram[:3]][ngram[3]] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('a', 440),\n",
            " ('that', 334),\n",
            " ('the', 318),\n",
            " ('not', 218),\n",
            " ('so', 115),\n",
            " ('just', 93),\n",
            " ('very', 80),\n",
            " ('for', 77),\n",
            " ('about', 73),\n",
            " ('an', 72)]\n"
          ]
        }
      ],
      "source": [
        "# an example n-1-gram with possible continuation words\n",
        "pprint(sorted(ngrams[('this', 'film', 'is')].items(), key=lambda x : x[1], reverse=True)[0:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looks good :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zL1VRNICdMi"
      },
      "source": [
        "# Task C\n",
        "\n",
        "* Generate new text, starting from `<bos> <bos> ...` (n-1 times) and ending after say 40 words, or `<eos>` being generated\n",
        "* I will give you a support function `sample_from` which receives a list of counts and a temperature parameter, and samples according to this distribution, returning a single column index drawn\n",
        "* The temperature sampling is described here: https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277\n",
        "* By all means, if you want to try, do try writing this function yourself!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD5D_DJbwc35",
        "outputId": "2a89022f-0db1-4a55-956f-ffebf77c4724"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy\n",
        "\n",
        "def softmax(x):\n",
        "    return numpy.exp(x)/sum(numpy.exp(x))\n",
        "\n",
        "def sample_from(counts,temperature=1.0):\n",
        "    \"\"\"\n",
        "    counts: list of counts that form the distribution\n",
        "    temperature: the \"how wild the generation should be\" parameter, numbers close\n",
        "                 to 0 are very conservative, numbers close or above 1 lead to quite\n",
        "                wild generations\n",
        "    \"\"\"\n",
        "\n",
        "    counts_array=numpy.array(counts)\n",
        "    #Make these sum up to 1.\n",
        "    counts_array_norm=counts_array/counts_array.sum()\n",
        "    #Divide by temperature, that is what the algorithm does\n",
        "    counts_array_norm/=temperature\n",
        "    #Renormalize into a distribution using the softmax function, that is what the algorithm does\n",
        "    final_distribution=softmax(counts_array_norm)\n",
        "    #A good way to sample from a distribution is the following function from numpy\n",
        "    x=numpy.random.multinomial(n=1,pvals=final_distribution)\n",
        "    selected_word=numpy.argmax(x).flatten()\n",
        "    return selected_word[0]\n",
        "\n",
        "sample_from([1,1,1,17],temperature=1.0) #Try running this several times each, with temps 0.1, 0.5, 1.0 ... see how temp 0.1 sticks to picking the max value, but higher temps don't?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLL6xqnzGRnT"
      },
      "source": [
        "# Task D: piece it all together\n",
        "\n",
        "* Again, I will give you the skeleton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRBDQI_1T0Hy",
        "outputId": "58759639-c770-4ecf-9e65-12c602320020"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Temp=0.1:\n",
            "'<bos> <bos> <bos> ELEPHANT WALK may not be anywhere I d love this <eos>'\n",
            "-----------\n",
            "Temp=0.5:\n",
            "('<bos> <bos> <bos> Eskimo is a skilled production that contains a debate I m '\n",
            " 'especially fond of Nestor Serrano s work in PUMPKINHEAD but effectively '\n",
            " 'grotesque when they pop up on his offer The result is weird And finally I '\n",
            " 'might have considered casting the beloved couple s real life discovery '\n",
            " 'Maureen O Hara looks lovely but seemingly has no connection or storyline')\n",
            "-----------\n",
            "Temp=1.0:\n",
            "('<bos> <bos> <bos> Anybody who wants to rediscover himself and the funds '\n",
            " 'inside the armored truck no one cares anymore Their daughter has been doing '\n",
            " 'his mojo since the 70s when it was screening at the Sundance film Festival '\n",
            " 'where I went when I was walking down the endless pristine and very white '\n",
            " 'corridors of a prison that looks much worse than The')\n",
            "-----------\n",
            "Temp=2.0:\n",
            "('<bos> <bos> <bos> Martha Plimpton has done some magnificent scores for films '\n",
            " 'of this era MJ Fox starts out in total confusion br br Sadly Outlaw of Gor '\n",
            " 'makes me wish an Atlas Shrugged film could have explored a serious change of '\n",
            " 'these characters supposed to have desired human kind so much that right '\n",
            " 'after that watch again the American war effort Fonda')\n",
            "-----------\n",
            "Temp=5.0:\n",
            "('<bos> <bos> <bos> Bedazzled is a far more emotional a drama rather that a '\n",
            " 'comedy on a dysfunctional royal family here living in the mall to avenge '\n",
            " 'himself by delivering a raw and emotion filled performance Facinelli not '\n",
            " 'only embraced his role with ease his daughter s role could be far reaching '\n",
            " 'and hurtful to our loved ones at the mercy of Imperial')\n",
            "-----------\n",
            "Temp=10.0:\n",
            "('<bos> <bos> <bos> Farhan Akhtar s in Dil Chahta Hai 2002 will be remembered '\n",
            " 'forever in movie history See it and act it out with equal abandon I m not '\n",
            " 'clueless I realize the main character Carly looked almost worthy of Kelly '\n",
            " 'Very few of them seem all the funnier because they provide a welcome respite '\n",
            " 'from all the amount of attention to')\n",
            "-----------\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def generate(ngrams,n,max_len=40,temperature=1.0,prompt=None):\n",
        "    \"\"\"\n",
        "    ngrams: the master dictionary\n",
        "    n: the n in n-gram\n",
        "    max_len: how many words max?\n",
        "    temperature: the generation temperature\n",
        "    prompt: the initial prompt, as a tuple, if not given n-1 <bos> symbols will be used\n",
        "    \"\"\"\n",
        "\n",
        "    if prompt is None:\n",
        "        prompt=[\"<bos>\"]*(n-1)\n",
        "\n",
        "    generated=list(prompt) #this list will grow with words\n",
        "    for _ in range(max_len):\n",
        "        sample_pos = sample_from([x[1] for x in list(ngrams[tuple(prompt)].items())])\n",
        "        next_word = [x[0] for x in list(ngrams[tuple(prompt)].items())][sample_pos]\n",
        "        generated.append(next_word)\n",
        "        prompt = list(generated[-3:])\n",
        "        if generated[-1]==\"<eos>\": #stop on end of sequence\n",
        "            break\n",
        "    return generated\n",
        "\n",
        "# Now we can test it!\n",
        "\n",
        "# make sure to match the n below to the n which was used to create\n",
        "# the master dictionary\n",
        "for temp in (0.1,0.5,1.0,2.0,5.0,10.0):\n",
        "    generated=generate(ngrams=ngrams,n=4,max_len=60,temperature=temp)\n",
        "    print(f\"Temp={temp}:\")\n",
        "    pprint(\" \".join(generated))\n",
        "    print(\"-----------\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMtAMOCJKA0z"
      },
      "source": [
        "# Done!\n",
        "\n",
        "Ok, the generations are quite funny. Clearly, this is no ChatGPT, but it is also not entirely bad for a model, which is basically two dictionaries..."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01a4d87ea5a54f16bbeae72894c85632": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21c661ddcf174e599811a3cd9513ca11": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cda0e27d1b6445b9b713e6eccd46012": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "64c545796eab4071903ef80275e2b2d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_696fae99698740249a27c31e20303c60",
            "placeholder": "​",
            "style": "IPY_MODEL_c14be0435fd84bdb8991b654c81b3045",
            "value": "100%"
          }
        },
        "696fae99698740249a27c31e20303c60": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cb8849da4eb4de5834cae0da4ac81ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_64c545796eab4071903ef80275e2b2d6",
              "IPY_MODEL_729e5eac79df4812a01397dbd3d3e22d",
              "IPY_MODEL_9b29d52a6b734c73ac8362c36fd8dd48"
            ],
            "layout": "IPY_MODEL_6ccc79e5d40d4438baf86cb38b2705af"
          }
        },
        "6ccc79e5d40d4438baf86cb38b2705af": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "729e5eac79df4812a01397dbd3d3e22d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9dfb87ed187143f0b6ca59d3fffd7224",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5cda0e27d1b6445b9b713e6eccd46012",
            "value": 3
          }
        },
        "9b29d52a6b734c73ac8362c36fd8dd48": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21c661ddcf174e599811a3cd9513ca11",
            "placeholder": "​",
            "style": "IPY_MODEL_01a4d87ea5a54f16bbeae72894c85632",
            "value": " 3/3 [00:00&lt;00:00, 84.16it/s]"
          }
        },
        "9dfb87ed187143f0b6ca59d3fffd7224": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c14be0435fd84bdb8991b654c81b3045": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
